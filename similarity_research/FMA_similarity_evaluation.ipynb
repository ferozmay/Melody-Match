{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FMA similarity evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def md(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD FILES\n",
    "# Load the main evaluator data\n",
    "main_evaluator_path = 'data/evaluation/FMA_similarity_evaluation_main_evaluator_N.tsv'\n",
    "main_evaluation = pd.read_csv(main_evaluator_path, sep='\\t')\n",
    "\n",
    "# Load the consensus evaluation data\n",
    "def preprocess_consensus_df(joined_evaluation):\n",
    "    '''Preprocess df for consensus evaluation'''\n",
    "    # Define relevant columns on the df\n",
    "    relevant_cols = ['Type', 'Track_id', 'Main_id']\n",
    "\n",
    "    # Pivot the table to have separate similarity columns for each evaluator\n",
    "    joined_pivot = joined_evaluation.pivot_table(index=relevant_cols,\n",
    "                                                columns='Evaluator',\n",
    "                                                values='Similarity',\n",
    "                                                aggfunc='first').reset_index()\n",
    "\n",
    "    # Rename similarity columns to include evaluator numbers\n",
    "    joined_pivot.columns = relevant_cols + [f'Evaluator{int(col+1)}' for col in range(3)]\n",
    "\n",
    "    # Remove \"main\" rows\n",
    "    joined_pivot = joined_pivot[joined_pivot['Type'] != 'Main'].reset_index(drop=True)\n",
    "\n",
    "    # Sort by main track id\n",
    "    joined_pivot = joined_pivot.sort_values(['Main_id', 'Type']).reset_index(drop=True)\n",
    "\n",
    "    return joined_pivot\n",
    "\n",
    "joined_evaluation_path = 'data/evaluation/FMA_similarity_evaluation_main16_joined.tsv'\n",
    "joined_evaluation_raw = pd.read_csv(joined_evaluation_path, sep='\\t')\n",
    "joined_evaluation = preprocess_consensus_df(joined_evaluation_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1 evaluator evaluated 80 tracks, one example for every 2 genres"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "3 evaluators evaluated the same 13 tracks"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Example of the evaluation of 1 track:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Track_id</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Link</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Title</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Album</th>\n",
       "      <th>Score</th>\n",
       "      <th>Main_id</th>\n",
       "      <th>Evaluator</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Main</td>\n",
       "      <td>20433</td>\n",
       "      <td>Main</td>\n",
       "      <td>https://melody.dvstr.net/song/20433</td>\n",
       "      <td>International</td>\n",
       "      <td>Drifting</td>\n",
       "      <td>Midival Punditz</td>\n",
       "      <td>Hello Hello</td>\n",
       "      <td>1.00</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Similar_1</td>\n",
       "      <td>133066</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nuevos ritos</td>\n",
       "      <td>Melange</td>\n",
       "      <td>Melange</td>\n",
       "      <td>0.95</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Similar_2</td>\n",
       "      <td>109473</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Music Industry Event</td>\n",
       "      <td>The Pink Tiles</td>\n",
       "      <td>Live on WFMU's Surface Noise with Joe McGasko:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Similar_3</td>\n",
       "      <td>66326</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E-Love</td>\n",
       "      <td>The Upsidedown</td>\n",
       "      <td>Dead Bees records label sampler #11</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Similar_4</td>\n",
       "      <td>148620</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decay of</td>\n",
       "      <td>2L8</td>\n",
       "      <td>The Answer</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Similar_5</td>\n",
       "      <td>64297</td>\n",
       "      <td>VS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Fantastic Doctrine</td>\n",
       "      <td>The New Mystikal Troubadours</td>\n",
       "      <td>III</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type  Track_id Similarity                                 Link  \\\n",
       "0       Main     20433       Main  https://melody.dvstr.net/song/20433   \n",
       "1  Similar_1    133066          S                                  NaN   \n",
       "2  Similar_2    109473          S                                  NaN   \n",
       "3  Similar_3     66326          N                                  NaN   \n",
       "4  Similar_4    148620          N                                  NaN   \n",
       "5  Similar_5     64297         VS                                  NaN   \n",
       "\n",
       "           Genre                   Title                        Artist  \\\n",
       "0  International                Drifting               Midival Punditz   \n",
       "1            NaN            Nuevos ritos                       Melange   \n",
       "2            NaN    Music Industry Event                The Pink Tiles   \n",
       "3            NaN                  E-Love                The Upsidedown   \n",
       "4            NaN                Decay of                           2L8   \n",
       "5            NaN  The Fantastic Doctrine  The New Mystikal Troubadours   \n",
       "\n",
       "                                               Album  Score  Main_id  \\\n",
       "0                                        Hello Hello   1.00    20433   \n",
       "1                                            Melange   0.95    20433   \n",
       "2  Live on WFMU's Surface Noise with Joe McGasko:...    NaN    20433   \n",
       "3                Dead Bees records label sampler #11   0.93    20433   \n",
       "4                                         The Answer   0.93    20433   \n",
       "5                                                III   0.93    20433   \n",
       "\n",
       "   Evaluator  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "5          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Example of the consensus evaluation of 1 track:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Track_id</th>\n",
       "      <th>Score</th>\n",
       "      <th>Main_id</th>\n",
       "      <th>Evaluator1</th>\n",
       "      <th>Evaluator2</th>\n",
       "      <th>Evaluator3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Similar_1</td>\n",
       "      <td>133066</td>\n",
       "      <td>0.95</td>\n",
       "      <td>20433</td>\n",
       "      <td>S</td>\n",
       "      <td>VS</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Similar_2</td>\n",
       "      <td>109473</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Similar_3</td>\n",
       "      <td>66326</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>N</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Similar_4</td>\n",
       "      <td>148620</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>N</td>\n",
       "      <td>D</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Similar_5</td>\n",
       "      <td>64297</td>\n",
       "      <td>0.93</td>\n",
       "      <td>20433</td>\n",
       "      <td>VS</td>\n",
       "      <td>S</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type  Track_id  Score  Main_id Evaluator1 Evaluator2 Evaluator3\n",
       "5  Similar_1    133066   0.95    20433          S         VS          S\n",
       "6  Similar_2    109473   0.93    20433        NaN        NaN          S\n",
       "7  Similar_3     66326   0.93    20433          N          S          S\n",
       "8  Similar_4    148620   0.93    20433          N          D          N\n",
       "9  Similar_5     64297   0.93    20433         VS          S          S"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BASIC EVALUATION\n",
    "def print_basic_info(main_evaluation, joined_evaluation):\n",
    "    '''Show basic info about what was evaluated'''\n",
    "    md(f\"1 evaluator evaluated {len(main_evaluation['Main_id'].unique())} tracks, one example for every 2 genres\")\n",
    "    md(f\"3 evaluators evaluated the same {len(joined_evaluation['Main_id'].unique())} tracks\")\n",
    "    md(\"Example of the evaluation of 1 track:\")\n",
    "    # TODO - Score is not always present nor is genre. I should add them from FMA_similarity_dict.json.gz\n",
    "    display(main_evaluation.head(6))\n",
    "    md(\"Example of the consensus evaluation of 1 track:\")\n",
    "    display(joined_evaluation[5:10])\n",
    "\n",
    "print_basic_info(main_evaluation, joined_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Similarity</th>\n",
       "      <th>count</th>\n",
       "      <th>%</th>\n",
       "      <th>Cumulative %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VS</td>\n",
       "      <td>59</td>\n",
       "      <td>14.75</td>\n",
       "      <td>14.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "      <td>196</td>\n",
       "      <td>49.00</td>\n",
       "      <td>63.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>66</td>\n",
       "      <td>16.50</td>\n",
       "      <td>80.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>79</td>\n",
       "      <td>19.75</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Similarity  count      %  Cumulative %\n",
       "0         VS     59  14.75         14.75\n",
       "1          S    196  49.00         63.75\n",
       "2          N     66  16.50         80.25\n",
       "3          D     79  19.75        100.00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# EVALUATE SIMILARITY ON MAIN EVALUATOR\n",
    "def evaluate_similarity(main_evaluation):\n",
    "    '''Evaluate similarity between the main evaluator on 80 tracks'''\n",
    "\n",
    "    # Get value counts % percentages for each similarity type \n",
    "    similar_tracks = main_evaluation[main_evaluation['Type'].str.contains(\"Similar\")]\n",
    "    counts = similar_tracks['Similarity'].value_counts(dropna=False).reset_index()\n",
    "    counts['%'] = counts['count'] / len(similar_tracks) * 100\n",
    "\n",
    "    # Order counts by similarity type\n",
    "    order = [\"VS\", \"S\", \"N\", \"D\"]\n",
    "    counts['Similarity'] = pd.Categorical(counts['Similarity'], categories=order, ordered=True)\n",
    "    counts = counts.sort_values('Similarity').reset_index(drop=True)\n",
    "\n",
    "    # Get cumulative percentage\n",
    "    counts['Cumulative %'] = counts['%'].cumsum()\n",
    "\n",
    "    display(counts)\n",
    "\n",
    "evaluate_similarity(main_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pairwise Agreement (Cohen's Kappa):\n",
      "Evaluator1 vs Evaluator2: 0.418\n",
      "Evaluator1 vs Evaluator3: 0.446\n",
      "Evaluator2 vs Evaluator3: 0.381\n",
      "\n",
      "Fleiss' Kappa (Overall Agreement for all evaluators): 0.414\n",
      "\n",
      "Full Agreement Percentage: 56.9%\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>Disagreements Table</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Contingency Matrix (Evaluator 1 vs Evaluator 2):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Evaluator2</th>\n",
       "      <th>D</th>\n",
       "      <th>N</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Evaluator1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Evaluator2  D  N   S\n",
       "Evaluator1          \n",
       "D           5  3   2\n",
       "N           2  6   4\n",
       "S           7  2  34"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chi-Square Test Results: χ² = 23.543, p-value = 0.00010\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def evaluate_agreement(joined_evaluation, group_vs_s=False):\n",
    "    \"\"\"\n",
    "    Evaluate agreement between evaluators using Cohen's Kappa, Fleiss' Kappa, \n",
    "    and measure overall agreement percentage. \n",
    "\n",
    "    Parameters:\n",
    "        joined_evaluation (pd.DataFrame): DataFrame containing similarity ratings from multiple evaluators.\n",
    "        group_vs_s (bool): If True, \"VS\" and \"S\" are treated as the same label.\n",
    "\n",
    "    Returns:\n",
    "        None (prints results and displays disagreement table)\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify evaluator columns\n",
    "    eval_cols = [col for col in joined_evaluation.columns if 'Evaluator' in col]\n",
    "\n",
    "    # Define rating mapping\n",
    "    if group_vs_s:\n",
    "        # Merge VS and S into one category\n",
    "        for col in eval_cols:\n",
    "            joined_evaluation[col] = joined_evaluation[col].replace({\"VS\": \"S\"})\n",
    "        rating_map = {\"S\": 2, \"N\": 1, \"D\": 0}  # Merging VS and S into one category\n",
    "    else:\n",
    "        rating_map = {\"VS\": 3, \"S\": 2, \"N\": 1, \"D\": 0}  # Keeping separate categories\n",
    "\n",
    "    # Convert ratings to numerical values\n",
    "    joined_evaluation_num = joined_evaluation.copy()\n",
    "    joined_evaluation_num[eval_cols] = joined_evaluation_num[eval_cols].map(lambda x: rating_map.get(x, np.nan))\n",
    "\n",
    "    ### 1. Compute Pairwise Agreement (Cohen’s Kappa)\n",
    "    pairwise_kappas = {}\n",
    "    for eval1, eval2 in combinations(eval_cols, 2):\n",
    "        kappa = cohen_kappa_score(joined_evaluation_num[eval1], joined_evaluation_num[eval2])\n",
    "        pairwise_kappas[f\"{eval1} vs {eval2}\"] = kappa\n",
    "\n",
    "    print(\"\\nPairwise Agreement (Cohen's Kappa):\")\n",
    "    for pair, score in pairwise_kappas.items():\n",
    "        print(f\"{pair}: {score:.3f}\")\n",
    "\n",
    "    ### 2. Compute Fleiss' Kappa\n",
    "    def compute_fleiss_kappa(df):\n",
    "        \"\"\"\n",
    "        Compute Fleiss' Kappa for multi-rater agreement.\n",
    "        :param df: DataFrame where each row represents an item rated by multiple evaluators.\n",
    "        :return: Fleiss' Kappa value.\n",
    "        \"\"\"\n",
    "        num_categories = len(set(rating_map.values()))  # Determine number of categories\n",
    "        rating_counts = np.zeros((len(df), num_categories))  \n",
    "\n",
    "        for i, row in enumerate(df.values):\n",
    "            for rating in row:\n",
    "                if not np.isnan(rating):\n",
    "                    rating_counts[i, int(rating)] += 1  \n",
    "\n",
    "        # Compute proportions\n",
    "        N, k = rating_counts.shape  \n",
    "        P_i = (np.sum(rating_counts**2, axis=1) - df.shape[1]) / (df.shape[1] * (df.shape[1] - 1))\n",
    "        P_bar = np.mean(P_i)\n",
    "        P_e = np.sum((np.sum(rating_counts, axis=0) / (N * df.shape[1]))**2)\n",
    "\n",
    "        return (P_bar - P_e) / (1 - P_e)\n",
    "\n",
    "    fleiss_kappa = compute_fleiss_kappa(joined_evaluation_num[eval_cols])\n",
    "    print(f\"\\nFleiss' Kappa (Overall Agreement for all evaluators): {fleiss_kappa:.3f}\")\n",
    "\n",
    "    ### 3. Identify Disagreements\n",
    "    joined_evaluation[\"Agreement\"] = joined_evaluation[eval_cols].nunique(axis=1) == 1  \n",
    "    agreement_percentage = joined_evaluation[\"Agreement\"].mean() * 100\n",
    "\n",
    "    print(f\"\\nFull Agreement Percentage: {agreement_percentage:.1f}%\")\n",
    "    disagreements = joined_evaluation[joined_evaluation[\"Agreement\"] == False]\n",
    "\n",
    "    # Display the disagreements table\n",
    "    md(\"<b>Disagreements Table</b>\")\n",
    "    # display(disagreements)\n",
    "\n",
    "    ### 4. Contingency Matrix\n",
    "    print(\"\\nContingency Matrix (Evaluator 1 vs Evaluator 2):\")\n",
    "    contingency_matrix = pd.crosstab(joined_evaluation[eval_cols[0]], joined_evaluation[eval_cols[1]])\n",
    "    display(contingency_matrix)\n",
    "\n",
    "    # Chi-Square Test for Dependency\n",
    "    chi2, p, _, _ = chi2_contingency(contingency_matrix)\n",
    "    print(f\"\\nChi-Square Test Results: χ² = {chi2:.3f}, p-value = {p:.5f}\")\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# evaluate_agreement(joined_evaluation, group_vs_s=False)  # Standard evaluation\n",
    "evaluate_agreement(joined_evaluation, group_vs_s=True)   # Evaluation with VS and S grouped"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

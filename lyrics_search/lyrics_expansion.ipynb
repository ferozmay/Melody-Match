{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Work with lyrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Make lyrics inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# LOAD DICTS\n",
    "lyrics_test_path = 'data/mxm_dataset_test.txt'\n",
    "lyrics_train_path = 'data/mxm_dataset_train.txt'\n",
    "\n",
    "# Output\n",
    "lyrics_all_words_path = 'data/lyrics_all_words.txt'\n",
    "lyrics_inverted_index_path = 'data/lyrics_inverted_idx.pkl'\n",
    "\n",
    "\n",
    "def read_lyrics(lyrics_path):\n",
    "    lyrics_dict = {}\n",
    "    with open(lyrics_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):\n",
    "                continue # It is a comment\n",
    "            if line.startswith('%'):\n",
    "                # List of all words\n",
    "                all_words = line[1:].split(',')\n",
    "            elif line.startswith('TR'):\n",
    "                line = line.split(',')\n",
    "                track_id = line[0]\n",
    "                word_dict = {int(id): int(freq) for id_freq in line[2:] for id, freq in [id_freq.split(':')]}\n",
    "                lyrics_dict[track_id] = word_dict\n",
    "    return lyrics_dict, all_words\n",
    "\n",
    "# Load the data\n",
    "train_lyrics_dict, all_words_train = read_lyrics(lyrics_train_path)\n",
    "test_lyrics_dict, all_words_test = read_lyrics(lyrics_test_path)\n",
    "\n",
    "# Ensure the list of words is the same\n",
    "assert all_words_train == all_words_test\n",
    "all_words = all_words_train\n",
    "\n",
    "# Make mappings from words to indices, where idx starts at 1\n",
    "word_to_idx = {word: index+1 for index, word in enumerate(all_words)}\n",
    "index_to_word = {index+1: word for index, word in enumerate(all_words)}\n",
    "\n",
    "# Join the dictionaries\n",
    "lyrics_dict_idx = {**train_lyrics_dict, **test_lyrics_dict}\n",
    "\n",
    "# Create a dictionary with words instead of indices\n",
    "lyrics_dict_word = {track_id: {index_to_word[int(id)]: int(freq) for id, freq in word_dict.items()} for track_id, word_dict in lyrics_dict_idx.items()}\n",
    "\n",
    "# Make inversed index\n",
    "def make_inveresed_index(lyrics_dict):\n",
    "    # Initialize an empty list for each word in the vocabulary\n",
    "    inverted_index = {word: {} for word in all_words}\n",
    "    \n",
    "    # Iterate over each track and its corresponding word dictionary\n",
    "    for track_id, word_dict in lyrics_dict.items():\n",
    "        # Iterate over each word ID in the word dictionary\n",
    "        for word_id, count in word_dict.items():\n",
    "            # Get the word corresponding to the word ID and append the track ID\n",
    "            word = index_to_word[word_id]\n",
    "            inverted_index[word][track_id] = count\n",
    "    \n",
    "    return inverted_index\n",
    "\n",
    "inverted_index = make_inveresed_index(lyrics_dict_idx)\n",
    "\n",
    "def save_lyrics_inverted_index(lyrics_inverted_index, path='data/lyrics_inverted_idx.pkl'):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(lyrics_inverted_index, file)\n",
    "    print(f\"lyrics_inverted_index saved as pickle at {path}\")\n",
    "\n",
    "# Save the inverted index\n",
    "save_lyrics_inverted_index(inverted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 5000\n",
      "Number of tracks: 237662\n",
      "First 5 words: ['i', 'the', 'you', 'to', 'and']\n",
      "One example of the index dictionary\n",
      "\tTRAAAAV128F421A322: {1: 6, 2: 4, 3: 2, 4: 2, 5: 5, 6: 3, 7: 1, 8: 1, 11: 1, 12: 2, 13: 3, 14: 1, 15: 1, 18: 2, 19: 2, 20: 2, 21: 2, 23: 4, 25: 1, 26: 2, 28: 1, 30: 1, 36: 2, 42: 1, 45: 1, 54: 2, 56: 1, 57: 1, 68: 1, 99: 1, 192: 2, 249: 1, 264: 1, 356: 1, 389: 1, 561: 1, 639: 1, 656: 1, 687: 1, 761: 1, 773: 1, 804: 1, 869: 2, 914: 1, 1035: 1, 1156: 1, 1221: 1, 1287: 1, 1364: 1, 1407: 1, 1533: 2, 1857: 1, 2096: 1, 2117: 1, 2482: 2, 2548: 1, 2705: 1, 2723: 1, 2868: 2, 2992: 2, 3455: 1, 3717: 1, 3851: 1, 4322: 1, 4382: 1, 4613: 1, 4713: 1, 4906: 1}\n",
      "One example of the word dictionary\n",
      "\tTRAAAAV128F421A322: {'i': 6, 'the': 4, 'you': 2, 'to': 2, 'and': 5, 'a': 3, 'me': 1, 'it': 1, 'my': 1, 'is': 2, 'of': 3, 'your': 1, 'that': 1, 'are': 2, 'we': 2, 'am': 2, 'will': 2, 'for': 4, 'be': 1, 'have': 2, 'so': 1, 'this': 1, 'like': 2, 'de': 1, 'up': 1, 'was': 2, 'if': 1, 'got': 1, 'would': 1, 'been': 1, 'these': 2, 'seem': 1, 'someon': 1, 'understand': 1, 'pass': 1, 'river': 1, 'met': 1, 'piec': 1, 'damn': 1, 'worth': 1, 'flesh': 1, 'grace': 1, 'poor': 2, 'somehow': 1, 'ignor': 1, 'passion': 1, 'tide': 1, 'season': 1, 'seed': 1, 'resist': 1, 'order': 2, 'piti': 1, 'fashion': 1, 'grant': 1, 'captur': 2, 'ici': 1, 'soil': 1, 'patienc': 1, 'social': 2, 'highest': 2, 'slice': 1, 'leaf': 1, 'lifeless': 1, 'arrang': 1, 'wilder': 1, 'shark': 1, 'devast': 1, 'element': 1}\n",
      "One example of the inversed index\n",
      "love (73423 tracks): {'TRAAAED128E0783FAB': 11, 'TRAAAHJ128F931194C': 1, 'TRAAAHZ128E0799171': 2, 'TRAAAUC128F428716F': 2, 'TRAABHB12903CAFC2F': 1}\n"
     ]
    }
   ],
   "source": [
    "# SHOW DATA\n",
    "print('Number of words:', len(all_words))\n",
    "print('Number of tracks:', len(lyrics_dict_idx))\n",
    "print('First 5 words:', all_words[:5])\n",
    "print('One example of the index dictionary')\n",
    "for track_id, word_dict in lyrics_dict_idx.items():\n",
    "    print(f\"\\t{track_id}: {word_dict}\")\n",
    "    break\n",
    "print('One example of the word dictionary')\n",
    "for track_id, word_dict in lyrics_dict_word.items():\n",
    "    print(f\"\\t{track_id}: {word_dict}\")\n",
    "    break\n",
    "\n",
    "print('One example of the inversed index')\n",
    "print(f\"love ({len(inverted_index['love'])} tracks):\", {k: v for k, v in list(inverted_index['love'].items())[:5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For word embeddings, we will use FastText Pretrained Models, which handle out-of-vocabulary (OOV) words using subword information.\n",
    "\n",
    "We will use the english embeddings from fasttext, downloaded from [here](https://fasttext.cc/docs/en/crawl-vectors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL\n",
    "# Info: https://fasttext.cc/docs/en/crawl-vectors.html\n",
    "# !pip install fasttext\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "# !pip install PyStemmer\n",
    "\n",
    "# Import word2vec model\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "\n",
    "# Use same word preprocessing as match-api\n",
    "import sys\n",
    "sys.path.append('../match-api/')\n",
    "from utils.text_processing import process_text, normalize, tokenize_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 5000/5000 [39:56<00:00,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final save complete. 5000 words processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SAVE A DICTIONARY WITH SIMILAR WORDS FOR ALL 5000 WORDS IN LYRICS\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_lyrics_similarity_dict(all_words, embeddings, save_path='lyrics_similarity_dict.json', batch_size=100):\n",
    "    '''\n",
    "    Create a dictionary with similar tokens to those in the lyrics.\n",
    "    Similar tokens are those with score > 0.7.\n",
    "    Progress is saved every batch_size words.\n",
    "    If interrupted, resumes from the saved file.\n",
    "    '''\n",
    "\n",
    "    # Load existing dictionary if the file exists\n",
    "    if os.path.exists(save_path):\n",
    "        with open(save_path, 'r', encoding='utf-8') as f:\n",
    "            lyrics_similarity_dict = json.load(f)\n",
    "        processed_words = set(lyrics_similarity_dict.keys())\n",
    "        print(f\"Loaded {len(processed_words)} processed words from {save_path}\")\n",
    "    else:\n",
    "        lyrics_similarity_dict = {}\n",
    "        processed_words = set()\n",
    "\n",
    "    # Get words to process\n",
    "    words_to_process = set(all_words) - processed_words\n",
    "\n",
    "    # Iterate over remaining words with a progress bar\n",
    "    for i, word in enumerate(tqdm(words_to_process, desc=\"Processing words\", initial=len(processed_words), total=len(all_words))):\n",
    "        # Get all words with a similarity score of >0.7\n",
    "        similar_words = {word.lower() for score, word in embeddings.get_nearest_neighbors(word, k=20) if score > 0.7}\n",
    "        extra_tokens = set(normalize(similar_words)) & set(all_words) - {word}\n",
    "\n",
    "        # Add the similar words to the dictionary\n",
    "        lyrics_similarity_dict[word] = list(extra_tokens)  # Convert set to list for JSON compatibility\n",
    "\n",
    "        # Save the dictionary to file every batch_size iterations\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(lyrics_similarity_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Final save to ensure all data is written\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(lyrics_similarity_dict, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Final save complete. {len(lyrics_similarity_dict)} words processed.\")\n",
    "\n",
    "    return lyrics_similarity_dict\n",
    "\n",
    "lyrics_similarity_dict = create_lyrics_similarity_dict(all_words, ft, save_path='data/lyrics_similarity_dict.json', batch_size=100)\n",
    "\n",
    "# Function to save the lyrics_similarity_dict as a pickle file\n",
    "def save_lyrics_similarity_dict(lyrics_similarity_dict, path='data/lyrics_similarity_dict.pkl'):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(lyrics_similarity_dict, file)\n",
    "    print(f\"lyrics_similarity_dict saved as pickle at {path}\")\n",
    "\n",
    "# Save the lyrics_similarity_dict as a pickle for faster loading\n",
    "save_lyrics_similarity_dict(lyrics_similarity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standalone code for lyrics expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fasttext model loaded\n",
      "lyrics_similarity_dict loaded from pickle at data/lyrics_similarity_dict.pkl\n",
      "lyrics_inverted_index loaded from pickle at data/lyrics_inverted_idx.pkl\n",
      "Original tokens: {'reew', 'ewr', 'love'}\n",
      "Tokens not in lyrics: {'reew', 'ewr'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ador', 'love'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STANDALONE CODE\n",
    "import pickle\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "# Use same word preprocessing as match-api\n",
    "import sys\n",
    "sys.path.append('../match-api/')\n",
    "from utils.text_processing import normalize, process_text\n",
    "\n",
    "def load_lyrics_inverted_index(path='data/lyrics_inverted_idx.pkl'):\n",
    "    '''load the lyrics_inverted_index from a pickle file'''\n",
    "    with open(path, 'rb') as file:\n",
    "        lyrics_inverted_index = pickle.load(file)\n",
    "    print(f\"lyrics_inverted_index loaded from pickle at {path}\")\n",
    "    return lyrics_inverted_index\n",
    "\n",
    "def load_lyrics_similarity_dict(path='data/lyrics_similarity_dict.pkl'):\n",
    "    '''load the lyrics_similarity_dict from a pickle file'''\n",
    "    with open(path, 'rb') as file:\n",
    "        lyrics_similarity_dict = pickle.load(file)\n",
    "    print(f\"lyrics_similarity_dict loaded from pickle at {path}\")\n",
    "    return lyrics_similarity_dict\n",
    "\n",
    "def load_expansion_model_dicts(ft_path, lyrics_similarity_dict_path, lyrics_inverted_index_path):\n",
    "    # Load word2vec model\n",
    "    ft = fasttext.load_model(ft_path)\n",
    "    print(\"fasttext model loaded\")\n",
    "\n",
    "    lyrics_similarity_dict = load_lyrics_similarity_dict(lyrics_similarity_dict_path)\n",
    "    lyrics_inverted_index = load_lyrics_inverted_index(lyrics_inverted_index_path)\n",
    "\n",
    "    return ft, lyrics_similarity_dict, lyrics_inverted_index\n",
    "\n",
    "def expand_query(query, lyrics_similarity_dict: dict, embeddings, verbose=False):\n",
    "    '''\n",
    "    Given a query, expand it with similar tokens from the lyrics BOW.\n",
    "\n",
    "    First, check if the word is alrady in the lyrics_similarity_dict.\n",
    "    Otherwise, get the embeddings from fasttext and get the similar words\n",
    "    '''\n",
    "\n",
    "    # Convert query into a set of stemmed tokens\n",
    "    tokens = set(process_text(query).split())\n",
    "    \n",
    "    # Check for tokens not present in the lyrics\n",
    "    tokens_in_lyrics = tokens & set(lyrics_similarity_dict.keys())\n",
    "    unseen_tokens = tokens - tokens_in_lyrics\n",
    "\n",
    "    # Expand with seen tokens\n",
    "    expanded_tokens = tokens_in_lyrics\n",
    "    expanded_tokens |= {similar_token for token in tokens_in_lyrics for similar_token in lyrics_similarity_dict[token]}\n",
    "\n",
    "    # Expand with unseen tokens (if present)\n",
    "    if unseen_tokens:\n",
    "        for token in unseen_tokens:\n",
    "            # Get similar words for unseen tokens\n",
    "            similar_words = {word.lower() for score, word in embeddings.get_nearest_neighbors(token, k=20) if score > 0.7}\n",
    "\n",
    "            # Normalize the words\n",
    "            similar_tokens = {token for token in normalize(similar_words) if token in lyrics_similarity_dict.keys()}\n",
    "\n",
    "            # Add the similar tokens to the set of tokens\n",
    "            expanded_tokens |= similar_tokens\n",
    "\n",
    "    if verbose:\n",
    "        print('Original tokens:', tokens)\n",
    "        print('Tokens not in lyrics:', unseen_tokens)\n",
    "    \n",
    "    # Return the expanded query\n",
    "    return expanded_tokens\n",
    "\n",
    "\n",
    "# Load all required models and dicts\n",
    "ft, lyrics_similarity_dict, lyrics_inverted_index = load_expansion_model_dicts('cc.en.300.bin', \n",
    "                                                                               'data/lyrics_similarity_dict.pkl', \n",
    "                                                                               'data/lyrics_inverted_idx.pkl')\n",
    "\n",
    "# Example query\n",
    "query = 'reew ewre love'\n",
    "expanded_tokens = expand_query(query, lyrics_similarity_dict, ft, verbose=True)\n",
    "expanded_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
